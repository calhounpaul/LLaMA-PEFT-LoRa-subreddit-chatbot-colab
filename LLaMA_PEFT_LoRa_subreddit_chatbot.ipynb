{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calhounpaul/LLaMA_PEFT_LoRa_subreddit_chatbot/blob/main/LLaMA_PEFT_LoRa_subreddit_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c0047e0-029e-4afe-b623-68ed4ea57a30",
      "metadata": {
        "id": "9c0047e0-029e-4afe-b623-68ed4ea57a30"
      },
      "source": [
        "This notebook demonstrates Parameter Efficient Fine Tuning (PEFT) for creating chatbot by training Facebook's publicly available LLaMA Large Language Model (LLM) on a public corpus categorized by topic (subreddit comments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5ef5a7f-5af6-4b69-afb3-7799059a46c1",
      "metadata": {
        "id": "d5ef5a7f-5af6-4b69-afb3-7799059a46c1"
      },
      "outputs": [],
      "source": [
        "import os, random, json, time, shutil, sys"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l-9UXpu9rasW"
      },
      "id": "l-9UXpu9rasW"
    },
    {
      "cell_type": "markdown",
      "id": "84e2ef13-3a94-4c37-98fb-168fd1369a16",
      "metadata": {
        "id": "84e2ef13-3a94-4c37-98fb-168fd1369a16"
      },
      "source": [
        "You can change the model repo below if you've got a sufficiently large GPU. Decapoda put all four (7b, 13b, 30b[1] and 65b) versions on huggingface.\n",
        "\n",
        "[1] *should be 33b, but isn't, due to an immortalized typo in the original facebook repo*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebd9490a-b0d5-4168-8ef9-f7ca64a98627",
      "metadata": {
        "id": "ebd9490a-b0d5-4168-8ef9-f7ca64a98627"
      },
      "outputs": [],
      "source": [
        "MODEL_REPO = \"decapoda-research/llama-7b-hf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b67ab2-50d3-4bb0-8006-3c4cd7006421",
      "metadata": {
        "id": "80b67ab2-50d3-4bb0-8006-3c4cd7006421"
      },
      "outputs": [],
      "source": [
        "THIS_DIR = os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95b42b6a-efc9-406e-9864-388d50e485fa",
      "metadata": {
        "id": "95b42b6a-efc9-406e-9864-388d50e485fa"
      },
      "source": [
        "Choose the subreddit you'd like to bring to life:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e227f126-ed86-40f6-927d-0b1cc84ebffa",
      "metadata": {
        "id": "e227f126-ed86-40f6-927d-0b1cc84ebffa"
      },
      "outputs": [],
      "source": [
        "SUBREDDIT_NAME = \"wholesome\"\n",
        "START_YEAR = 2022\n",
        "START_MONTH = 1\n",
        "END_YEAR = 2022\n",
        "END_MONTH = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "013761ee-bb0e-45da-8e47-14eafa170b2d",
      "metadata": {
        "id": "013761ee-bb0e-45da-8e47-14eafa170b2d"
      },
      "source": [
        "We're going to use magnusnissel/ps_reddit_tool to download *very* large chunks of reddit archives. The submissions and comments from a single month can be upwards of 30GB, so expect to wait a while for both download and extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8625509-7276-4693-a18e-e585b17c6e04",
      "metadata": {
        "id": "f8625509-7276-4693-a18e-e585b17c6e04"
      },
      "outputs": [],
      "source": [
        "%cd {THIS_DIR}\n",
        "!git clone https://github.com/magnusnissel/ps_reddit_tool\n",
        "%cd ps_reddit_tool\n",
        "!git checkout 0cb0bfd\n",
        "!pip install --upgrade -qq -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf2180aa-9020-4a7b-a588-3439c528b808",
      "metadata": {
        "id": "bf2180aa-9020-4a7b-a588-3439c528b808"
      },
      "source": [
        "Since many of you will be using colab, now is a good time to establish some consolidated save points so the whole archive doesn't need to be downloaded repeatedly just to get the same list of comments.\n",
        "\n",
        "It may also be more convenient to mount a google drive and load everything from there.\n",
        "\n",
        "First, determine where the massive reddit files will go prior to extraction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3adb3d3c-447d-4724-8d6e-d67186c737de",
      "metadata": {
        "id": "3adb3d3c-447d-4724-8d6e-d67186c737de"
      },
      "outputs": [],
      "source": [
        "REDDIT_LOC = os.path.join(THIS_DIR,\"thisdayinreddit\") #change to suit your needs\n",
        "os.makedirs(REDDIT_LOC,exist_ok = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f081fb69-43e7-4e09-8c16-5f9dad3c6628",
      "metadata": {
        "id": "f081fb69-43e7-4e09-8c16-5f9dad3c6628"
      },
      "outputs": [],
      "source": [
        "!python3 cli.py config folder {REDDIT_LOC}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33c24bb5-6e22-4596-ad19-6d3ed10f0e38",
      "metadata": {
        "id": "33c24bb5-6e22-4596-ad19-6d3ed10f0e38"
      },
      "source": [
        "ps_reddit_tool allows for batch downloading, but we're going to do it in a loop so that we can skip whatever is already uploaded to {REDDIT_LOC}/extracted. So, if you're using a colab instance (hopefully not one with a GPU yet, since training hasn't started) you should try to download the extracted .json files in {REDDIT_LOC}/extracted/monthly/{SUBREDDIT_NAME} as they appear. That way, you can re-upload them if you lose the instance, and they'll be skipped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a1032cc-5674-4252-9eef-2aa400eae571",
      "metadata": {
        "id": "9a1032cc-5674-4252-9eef-2aa400eae571"
      },
      "outputs": [],
      "source": [
        "completed_extractions_loc = os.path.join(REDDIT_LOC,\"extracted\",\"monthly\",SUBREDDIT_NAME)\n",
        "os.makedirs(completed_extractions_loc,exist_ok = True)\n",
        "print(completed_extractions_loc)\n",
        "zipped_archives_loc = os.path.join(REDDIT_LOC,\"compressed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83efcaaf-656e-43c6-87c1-180935394b5d",
      "metadata": {
        "id": "83efcaaf-656e-43c6-87c1-180935394b5d"
      },
      "source": [
        "If you're restarting, you should stop here and drop the json files into the completed extractions directory printed above. Otherwise, you can just move on.\n",
        "\n",
        "Here's a toggle to choose if you'd like to delete the reddit archives from disk after download and extraction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8789dd31-3702-48c8-a79c-54031641c4aa",
      "metadata": {
        "id": "8789dd31-3702-48c8-a79c-54031641c4aa"
      },
      "outputs": [],
      "source": [
        "DELETE_ARCHIVES_AFTER_EXTRACTION = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22fd2356-bd1e-4c8b-85c6-23f5f64fdab1",
      "metadata": {
        "id": "22fd2356-bd1e-4c8b-85c6-23f5f64fdab1"
      },
      "outputs": [],
      "source": [
        "completed_extractions_filenames = os.listdir(completed_extractions_loc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fd7e773-bb91-4108-bfc7-545c2ed703fd",
      "metadata": {
        "id": "7fd7e773-bb91-4108-bfc7-545c2ed703fd"
      },
      "outputs": [],
      "source": [
        "for this_year in range(START_YEAR,END_YEAR+1):\n",
        "    this_start_month = START_MONTH if this_year == START_YEAR else 1\n",
        "    this_end_month = END_MONTH if this_year == END_YEAR else 12\n",
        "    for this_month in range(this_start_month,this_end_month+1):\n",
        "        print(\"MONTH\",this_month,\"YEAR\",this_year)\n",
        "        date_str = str(this_year)+\"-\"+str(this_month).zfill(2)\n",
        "        filename_tail = \"_\"+SUBREDDIT_NAME+date_str+\".json\"\n",
        "        if \"RS\"+filename_tail not in completed_extractions_filenames:\n",
        "            !python3 cli.py submissions download {date_str} {date_str} # --force=True\n",
        "            !python3 cli.py submissions extract {date_str} {date_str} {SUBREDDIT_NAME}\n",
        "        if \"RC\"+filename_tail not in completed_extractions_filenames:\n",
        "            !python3 cli.py comments download {date_str} {date_str} # --force=True\n",
        "            !python3 cli.py comments extract {date_str} {date_str} {SUBREDDIT_NAME}\n",
        "        if DELETE_ARCHIVES_AFTER_EXTRACTION:\n",
        "            shutil.rmtree(zipped_archives_loc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "162f2a4a-f34a-461b-8518-8647433b8d6a",
      "metadata": {
        "id": "162f2a4a-f34a-461b-8518-8647433b8d6a"
      },
      "source": [
        "Your chosen subreddit's extracted data should now be locally available in JSON files that can be used to construct linear conversations for training an LLM to behave as a chatbot.\n",
        "\n",
        "We'll be using redditcleaner to clean the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a80a6e1-f3c4-4da1-abc5-9d19af113eee",
      "metadata": {
        "id": "8a80a6e1-f3c4-4da1-abc5-9d19af113eee"
      },
      "outputs": [],
      "source": [
        "!pip3 install -qq redditcleaner\n",
        "import redditcleaner"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4dc64a7-a8d1-4cfc-86f3-27d96a335f34",
      "metadata": {
        "id": "b4dc64a7-a8d1-4cfc-86f3-27d96a335f34"
      },
      "source": [
        "Now load the submission and comment lists into memory. Everything will be linked up in a moment. There's probably a cleaner way to do this, but it already takes a negligible amount of time relative to the rest of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85a87f4f-9629-4a13-8ab8-04af9ea3a6b1",
      "metadata": {
        "id": "85a87f4f-9629-4a13-8ab8-04af9ea3a6b1"
      },
      "outputs": [],
      "source": [
        "comments = []\n",
        "submissions = []\n",
        "\n",
        "for filename in os.listdir(completed_extractions_loc):\n",
        "    if not filename.endswith(\".json\"):\n",
        "        continue\n",
        "    with open(os.path.join(completed_extractions_loc,filename),\"r\") as f:\n",
        "        try:\n",
        "            if filename.startswith(\"RC\"):\n",
        "                comments += json.load(f)\n",
        "            elif filename.startswith(\"RS\"):\n",
        "                submissions+= json.load(f)\n",
        "        except:\n",
        "            print(filename,\"failed\")\n",
        "            pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31a81f49-69c7-4d25-8033-ae1ff6912831",
      "metadata": {
        "id": "31a81f49-69c7-4d25-8033-ae1ff6912831"
      },
      "source": [
        "Now organize submissions and comments by id:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e59392f-5f6f-4a7b-b791-7e385b4ea481",
      "metadata": {
        "id": "2e59392f-5f6f-4a7b-b791-7e385b4ea481"
      },
      "outputs": [],
      "source": [
        "submissions_by_id = {}\n",
        "for sub in submissions:\n",
        "    submissions_by_id[sub[\"id\"]] = sub\n",
        "\n",
        "comments_by_id = {}\n",
        "for comment in comments:\n",
        "    comments_by_id[comment[\"id\"]] = comment\n",
        "    try:\n",
        "        comments_by_id[comment[\"id\"]][\"pid\"]=comment[\"parent_id\"].split(\"_\")[-1]\n",
        "    except:\n",
        "        comments_by_id[comment[\"id\"]][\"pid\"]= None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fc5da19-86fa-4d78-bc2d-92af8c91f5ee",
      "metadata": {
        "id": "5fc5da19-86fa-4d78-bc2d-92af8c91f5ee"
      },
      "source": [
        "Construct lists of parent comment IDs and childless comment IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20d96d34-c19a-4b71-a935-152089bbd1d4",
      "metadata": {
        "id": "20d96d34-c19a-4b71-a935-152089bbd1d4"
      },
      "outputs": [],
      "source": [
        "parents_list = set()\n",
        "for id in comments_by_id:\n",
        "    if comments_by_id[id][\"pid\"]:\n",
        "        parents_list.add(comments_by_id[id][\"pid\"])\n",
        "\n",
        "parents_list = list(parents_list)\n",
        "\n",
        "leaf_list = set()\n",
        "for id in comments_by_id:\n",
        "    if comments_by_id[id][\"pid\"]:\n",
        "        if id not in parents_list:\n",
        "            leaf_list.add(id)\n",
        "\n",
        "leaf_list = list(leaf_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "383e4af1-ec9a-4ce9-bf4e-e8712f1ed93f",
      "metadata": {
        "id": "383e4af1-ec9a-4ce9-bf4e-e8712f1ed93f"
      },
      "source": [
        "Each childless comment forms a unique chat conversation (with the original reddit submission as the first message) that can be followed upstream to the initial parent, which can then be represented in the chat as a reply to the first message. (note: initial comments will be overrepresented with this method)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0c4889-e11c-4339-9603-4d45965c0cf7",
      "metadata": {
        "id": "9d0c4889-e11c-4339-9603-4d45965c0cf7"
      },
      "outputs": [],
      "source": [
        "chats_list = []\n",
        "for leaf_id in leaf_list:\n",
        "    this_chat = [leaf_id,]\n",
        "    if leaf_id not in comments_by_id:\n",
        "        continue\n",
        "    skip = False\n",
        "    while this_chat[0] in comments_by_id:\n",
        "        if comments_by_id[this_chat[0]][\"body\"] == \"[deleted]\" or \\\n",
        "            comments_by_id[this_chat[0]][\"author\"] == \"[deleted]\":\n",
        "            skip = True\n",
        "            break\n",
        "        this_chat = [comments_by_id[this_chat[0]][\"pid\"],] + this_chat\n",
        "    if skip:\n",
        "        continue\n",
        "    if this_chat[0] not in submissions_by_id:\n",
        "        continue\n",
        "    if len(this_chat)<3:\n",
        "        continue\n",
        "    if \"selftext\" in submissions_by_id[this_chat[0]]:\n",
        "        if submissions_by_id[this_chat[0]][\"selftext\"] == \"[removed]\":\n",
        "            continue\n",
        "    chats_list.append(this_chat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9961878a-ceea-49ab-9d89-30eba77d8b80",
      "metadata": {
        "id": "9961878a-ceea-49ab-9d89-30eba77d8b80"
      },
      "source": [
        "Now the list of chats (message lists) can be formatted according to preference. I chose <human_##> and <bot>.\n",
        "\n",
        "These constants determine formatting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2ae7cdc-df50-4c28-8e0f-a89014e4af01",
      "metadata": {
        "id": "e2ae7cdc-df50-4c28-8e0f-a89014e4af01"
      },
      "outputs": [],
      "source": [
        "name_separator = \": \"\n",
        "message_separator = \"\\n\"\n",
        "human_username_replacements = [\"<human_\" + str(n).zfill(2) + \">\" for n in range(200)]\n",
        "bot_username = \"<bot>\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ade2872-5870-4a11-a8f3-3c1deac9e1b8",
      "metadata": {
        "id": "3ade2872-5870-4a11-a8f3-3c1deac9e1b8"
      },
      "source": [
        "Comments don't have separate titles, but self posts do, so the first message for chats constructed from self posts will use a random one of these as a separator between the title and body:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d817947-bc72-47ea-8b2c-05ae008579dd",
      "metadata": {
        "id": "5d817947-bc72-47ea-8b2c-05ae008579dd"
      },
      "outputs": [],
      "source": [
        "title_body_separators = [\": \",\" - \",\"\\n\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dff4dc8d-a673-49c4-87b2-5c594d212273",
      "metadata": {
        "id": "dff4dc8d-a673-49c4-87b2-5c594d212273"
      },
      "source": [
        "clean_text_body can be modified for different purposes. I've set it up to just remove all double spaces and run redditcleaner on the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "021904fb-c09e-4399-ae70-8e6b55ee09a1",
      "metadata": {
        "id": "021904fb-c09e-4399-ae70-8e6b55ee09a1"
      },
      "outputs": [],
      "source": [
        "final_training_strings = []\n",
        "\n",
        "def clean_text_body(input_text):\n",
        "    while \"\\n\\n\" in input_text:\n",
        "        input_text = input_text.replace(\"\\n\\n\",\"\\n\")\n",
        "    input_text = redditcleaner.clean(input_text)\n",
        "    return input_text.strip()\n",
        "\n",
        "for chat in chats_list:\n",
        "    train_string = \"\"\n",
        "    submission_data = submissions_by_id[chat[0]]\n",
        "    authors_map = {submission_data[\"author\"]:human_username_replacements[0],}\n",
        "    train_string += authors_map[submission_data[\"author\"]]\n",
        "    train_string += name_separator\n",
        "    train_string += submission_data[\"title\"]\n",
        "    if submission_data[\"domain\"].startswith(\"self.\"):\n",
        "        if len(submission_data[\"selftext\"]):\n",
        "            if train_string.endswith(\".\"):\n",
        "                train_string = train_string[:-1]\n",
        "            train_string += random.choice(title_body_separators) + clean_text_body(submission_data[\"selftext\"])\n",
        "    elif \"url\" in submission_data:\n",
        "        train_string += random.choice(title_body_separators) + submission_data[\"url\"]\n",
        "    non_op_author_scores = {}\n",
        "    top_author_score = 0\n",
        "    top_scoring_author = None\n",
        "    names_iterator = 1\n",
        "    skip = False\n",
        "    for msg_id in chat[1:]:\n",
        "        msg = comments_by_id[msg_id]\n",
        "        if msg[\"author\"] == submission_data[\"author\"]:\n",
        "            continue\n",
        "        if msg[\"author\"] not in non_op_author_scores:\n",
        "            non_op_author_scores[msg[\"author\"]] = 0\n",
        "        if msg[\"author\"] not in authors_map:\n",
        "            authors_map[msg[\"author\"]] = human_username_replacements[names_iterator]\n",
        "            names_iterator+=1\n",
        "        non_op_author_scores[msg[\"author\"]] += msg[\"score\"]\n",
        "        if non_op_author_scores[msg[\"author\"]] > top_author_score:\n",
        "            top_author_score = non_op_author_scores[msg[\"author\"]]\n",
        "            top_scoring_author = msg[\"author\"]\n",
        "    if skip:\n",
        "        continue\n",
        "    if top_author_score < 0:\n",
        "        continue\n",
        "    authors_map[top_scoring_author] = bot_username\n",
        "    for msg_id in chat[1:]:\n",
        "        msg = comments_by_id[msg_id]\n",
        "        train_string+=message_separator+authors_map[msg[\"author\"]]+name_separator\n",
        "        train_string+=clean_text_body(msg[\"body\"])\n",
        "    final_training_strings.append({\"text\":train_string,})\n",
        "\n",
        "train_path = os.path.join(THIS_DIR,\"dataset.json\")\n",
        "with open(train_path,\"w\") as f:\n",
        "    json.dump(final_training_strings,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56007157-abf2-46d0-aa30-1e402671aa49",
      "metadata": {
        "id": "56007157-abf2-46d0-aa30-1e402671aa49"
      },
      "source": [
        "The training part is mostly modified code from the alpaca-lora repo: https://github.com/tloen/alpaca-lora\n",
        "\n",
        "I'll update it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "339bb113-e65a-49f4-a796-d63069a7786c",
      "metadata": {
        "id": "339bb113-e65a-49f4-a796-d63069a7786c"
      },
      "outputs": [],
      "source": [
        "!pip3 install --upgrade datasets\n",
        "!pip3 install --upgrade loralib\n",
        "!pip3 install --upgrade sentencepiece\n",
        "!pip3 install --upgrade git+https://github.com/huggingface/transformers.git\n",
        "!pip3 install --upgrade accelerate\n",
        "!pip3 install --upgrade bitsandbytes\n",
        "!pip3 install --upgrade git+https://github.com/huggingface/peft.git\n",
        "!pip3 install --upgrade gradio\n",
        "!pip3 install --upgrade appdirs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8f091e9-a2d9-4e00-8b1f-2c2638ef615b",
      "metadata": {
        "id": "a8f091e9-a2d9-4e00-8b1f-2c2638ef615b"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import fire\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from datasets import load_dataset\n",
        "import transformers\n",
        "\n",
        "assert (\n",
        "    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n",
        "), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer, GenerationConfig\n",
        "from peft import (\n",
        "    prepare_model_for_int8_training,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    get_peft_model_state_dict,\n",
        "    PeftModel,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d3cb031-9e3c-4e08-8f5f-d25d963181bb",
      "metadata": {
        "id": "4d3cb031-9e3c-4e08-8f5f-d25d963181bb"
      },
      "source": [
        "I recommend trying this multiple times and varying your hyperparams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50d8fed3-5ed4-425a-b396-29062444f722",
      "metadata": {
        "id": "50d8fed3-5ed4-425a-b396-29062444f722"
      },
      "outputs": [],
      "source": [
        "# model/data params\n",
        "base_model = MODEL_REPO  # the only required argument\n",
        "data_path = train_path\n",
        "output_dir = os.path.join(THIS_DIR,SUBREDDIT_NAME+\"_llama\")\n",
        "# training hyperparams\n",
        "batch_size = 128\n",
        "micro_batch_size = 4\n",
        "num_epochs = 3\n",
        "learning_rate = 1e-4\n",
        "cutoff_len = 512\n",
        "val_set_size = 200\n",
        "# lora hyperparams\n",
        "lora_r = 8\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.05\n",
        "lora_target_modules = [\n",
        "    \"q_proj\",\n",
        "    \"v_proj\",\n",
        "]\n",
        "# llm hyperparams\n",
        "train_on_inputs = True  # if False, masks out inputs in loss\n",
        "group_by_length = True  # faster, but produces an odd training loss curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19e951df-0fe5-4aac-a71e-4a8b670c94ba",
      "metadata": {
        "id": "19e951df-0fe5-4aac-a71e-4a8b670c94ba"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    f\"Training Alpaca-LoRA model with params:\\n\"\n",
        "    f\"base_model: {base_model}\\n\"\n",
        "    f\"data_path: {data_path}\\n\"\n",
        "    f\"output_dir: {output_dir}\\n\"\n",
        "    f\"batch_size: {batch_size}\\n\"\n",
        "    f\"micro_batch_size: {micro_batch_size}\\n\"\n",
        "    f\"num_epochs: {num_epochs}\\n\"\n",
        "    f\"learning_rate: {learning_rate}\\n\"\n",
        "    f\"cutoff_len: {cutoff_len}\\n\"\n",
        "    f\"val_set_size: {val_set_size}\\n\"\n",
        "    f\"lora_r: {lora_r}\\n\"\n",
        "    f\"lora_alpha: {lora_alpha}\\n\"\n",
        "    f\"lora_dropout: {lora_dropout}\\n\"\n",
        "    f\"lora_target_modules: {lora_target_modules}\\n\"\n",
        "    f\"train_on_inputs: {train_on_inputs}\\n\"\n",
        "    f\"group_by_length: {group_by_length}\\n\"\n",
        ")\n",
        "\n",
        "gradient_accumulation_steps = batch_size // micro_batch_size\n",
        "\n",
        "device_map = \"auto\"\n",
        "world_size = 1\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    load_in_8bit=True,\n",
        "    device_map=device_map,\n",
        ")\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
        "\n",
        "tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
        "tokenizer.padding_side = \"left\"  # Allow batched inference\n",
        "\n",
        "def tokenize(prompt, add_eos_token=True):\n",
        "    result = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=cutoff_len,\n",
        "        padding=False,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    if (\n",
        "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
        "        and len(result[\"input_ids\"]) < cutoff_len\n",
        "        and add_eos_token\n",
        "    ):\n",
        "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
        "        result[\"attention_mask\"].append(1)\n",
        "\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "    return result\n",
        "\n",
        "def generate_and_tokenize_prompt(data_point):\n",
        "    full_prompt = data_point[\"text\"]\n",
        "    tokenized_full_prompt = tokenize(full_prompt)\n",
        "    if not train_on_inputs:\n",
        "        user_prompt = generate_prompt({**data_point, \"output\": \"\"})\n",
        "        tokenized_user_prompt = tokenize(user_prompt, add_eos_token=False)\n",
        "        user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n",
        "\n",
        "        tokenized_full_prompt[\"labels\"] = [\n",
        "            -100\n",
        "        ] * user_prompt_len + tokenized_full_prompt[\"labels\"][\n",
        "            user_prompt_len:\n",
        "        ]  # could be sped up, probably\n",
        "    return tokenized_full_prompt\n",
        "\n",
        "model = prepare_model_for_int8_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=lora_r,\n",
        "    lora_alpha=lora_alpha,\n",
        "    target_modules=lora_target_modules,\n",
        "    lora_dropout=lora_dropout,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "data = load_dataset(\"json\", data_files=data_path)\n",
        "\n",
        "if val_set_size > 0:\n",
        "    train_val = data[\"train\"].train_test_split(\n",
        "        test_size=val_set_size, shuffle=True, seed=42\n",
        "    )\n",
        "    train_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
        "    val_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
        "else:\n",
        "    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
        "    val_data = None\n",
        "\n",
        "from transformers import TrainerCallback\n",
        "\n",
        "#class SaveCallback(TrainerCallback):\n",
        "#    def on_step_end(self, args, state, control, **kwargs):\n",
        "#        if state.global_step % interval == 0:\n",
        "#           pass\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e3439f2-1a71-4774-9bf8-755f28c7fd14",
      "metadata": {
        "id": "2e3439f2-1a71-4774-9bf8-755f28c7fd14"
      },
      "outputs": [],
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=micro_batch_size,\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "        warmup_steps=10,\n",
        "        num_train_epochs=num_epochs,\n",
        "        learning_rate=learning_rate,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
        "        save_strategy=\"steps\",\n",
        "        eval_steps=10 if val_set_size > 0 else None,\n",
        "        save_steps=100,\n",
        "        output_dir=output_dir,\n",
        "        save_total_limit=3,\n",
        "        load_best_model_at_end=True if val_set_size > 0 else False,\n",
        "        group_by_length=group_by_length,\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForSeq2Seq(\n",
        "        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
        "    ),\n",
        "    #callbacks=[SaveCallback],\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "old_state_dict = model.state_dict\n",
        "model.state_dict = (\n",
        "    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n",
        ").__get__(model, type(model))\n",
        "\n",
        "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "    model = torch.compile(model)\n",
        "\n",
        "resume_from_checkpoint = False\n",
        "if os.path.exists(output_dir):\n",
        "    if len([n for n in os.listdir(output_dir) if n.startswith(\"checkpoint\")]):\n",
        "        resume_from_checkpoint = True\n",
        "trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "\n",
        "model.save_pretrained(output_dir)\n",
        "\n",
        "print(\"\\n If there's a warning about missing keys above, please disregard :)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92b256e9-d9c0-41b4-b9ea-94e889bc1125",
      "metadata": {
        "id": "92b256e9-d9c0-41b4-b9ea-94e889bc1125"
      },
      "source": [
        "The PEFT weights are efficient because they store only the fine-tuned parameters (hence the name). In theory, this could allow a single pretrained LLM to have many small hot-swappable fine-tuned files specialized for different tasks, e.g. one for self-reflection, one for sentiment analysis, one for summarization, etc.\n",
        "\n",
        "(I demonstrated some of GPT-Neox-20B's few-shot abilities here: https://github.com/calhounpaul/GPT-NeoX-20B-8bit-inference, so task-specific fine-tuning should mostly be a matter of pushing the network into the right state to make use of an existing talent).\n",
        "\n",
        "This next part reintegrates the PEFT weights into the original model for uploading to HF. This lets other people use it directly, without bothering with PEFT. You can also push the PEFT weights alone, which saves a lot of bandwidth for people who already have LLaMA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f380a8-5377-4d86-92a7-7ba9926d0dff",
      "metadata": {
        "id": "29f380a8-5377-4d86-92a7-7ba9926d0dff"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "\n",
        "base_model = LlamaForCausalLM.from_pretrained(\n",
        "    MODEL_REPO,\n",
        "    load_in_8bit=False,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": \"cpu\"},\n",
        ")\n",
        "\n",
        "first_weight = base_model.model.layers[0].self_attn.q_proj.weight\n",
        "first_weight_old = first_weight.clone()\n",
        "\n",
        "lora_model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    output_dir,\n",
        "    device_map={\"\": \"cpu\"},\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "lora_weight = lora_model.base_model.model.model.layers[0].self_attn.q_proj.weight\n",
        "\n",
        "assert torch.allclose(first_weight_old, first_weight)\n",
        "\n",
        "# merge weights\n",
        "for layer in lora_model.base_model.model.model.layers:\n",
        "    layer.self_attn.q_proj.merge_weights = True\n",
        "    layer.self_attn.v_proj.merge_weights = True\n",
        "\n",
        "lora_model.train(False)\n",
        "\n",
        "# did we do anything?\n",
        "assert not torch.allclose(first_weight_old, first_weight)\n",
        "\n",
        "lora_model_sd = lora_model.state_dict()\n",
        "deloreanized_sd = {\n",
        "    k.replace(\"base_model.model.\", \"\"): v\n",
        "    for k, v in lora_model_sd.items()\n",
        "    if \"lora\" not in k\n",
        "}\n",
        "\n",
        "new_hf_model_ckpt = os.path.join(THIS_DIR,\"hf_ckpt\")\n",
        "LlamaForCausalLM.save_pretrained(\n",
        "    base_model, new_hf_model_ckpt, state_dict=deloreanized_sd, max_shard_size=\"400MB\"\n",
        ")\n",
        "\n",
        "print(\"HF model exported to\",new_hf_model_ckpt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9c9c062-65be-476a-bfb0-2436b59413f6",
      "metadata": {
        "id": "b9c9c062-65be-476a-bfb0-2436b59413f6"
      },
      "source": [
        "Now you have a fully-HF-compatible model that can be shared on the hub: https://huggingface.co/docs/transformers/model_sharing\n",
        "\n",
        "Here's a very rudimentary gradio chatbot interface:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b99c4eb-5bce-4973-a285-fe056b93d336",
      "metadata": {
        "id": "2b99c4eb-5bce-4973-a285-fe056b93d336"
      },
      "outputs": [],
      "source": [
        "CREATE_TEMPORARY_PUBLIC_CHATBOT_URL_FOR_SHARING = True\n",
        "\n",
        "import gradio\n",
        "\n",
        "del base_model\n",
        "#del model\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    os.path.join(THIS_DIR,\"hf_ckpt\"),\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0} #\"auto\",\n",
        ")\n",
        "# unwind broken decapoda-research config\n",
        "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
        "model.config.bos_token_id = 1\n",
        "model.config.eos_token_id = 2\n",
        "\n",
        "model.eval()\n",
        "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
        "    model = torch.compile(model)\n",
        "def evaluate(\n",
        "        prompt,\n",
        "        eos_strings =[\"<human\",\"<bot>:\"],\n",
        "        temperature=0.1,\n",
        "        top_p=0.75,\n",
        "        top_k=40,\n",
        "        num_beams=4,\n",
        "        max_new_tokens=128,\n",
        "        **kwargs,\n",
        "    ):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        num_beams=num_beams,\n",
        "        **kwargs,\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "    s = generation_output.sequences[0]\n",
        "    output = tokenizer.decode(s)\n",
        "    with open(\"logs/\" + str(time.time()) + \".txt\",\"w\") as f:\n",
        "        f.write(\"prompt: \"+prompt+\"\\n\\noutput: \"+output)\n",
        "    new_generated_part = output[len(prompt)+1:]\n",
        "    for eos_str in eos_strings:\n",
        "        new_generated_part = new_generated_part.split(eos_str)[0].strip()\n",
        "    return new_generated_part\n",
        "os.makedirs(\"logs\",exist_ok=True)\n",
        "def chat_cb(latest_human,history=[]):\n",
        "    conv_string = \"\"\n",
        "    for conv_pair in history:\n",
        "        human_msg = BeautifulSoup(conv_pair[0]).text\n",
        "        bot_msg = BeautifulSoup(conv_pair[1]).text\n",
        "        conv_string += \"<human_00>: \" + conv_pair[0] + \"\\n<bot>: \" + conv_pair[1] + \"\\n\"\n",
        "    conv_string += \"<human_00>: \" + latest_human + \"\\n<bot>:\"\n",
        "    bot_reply = evaluate(conv_string).replace(\"\\n\",\"<br>\")\n",
        "    history.append([latest_human.replace(\"\\n\",\"<br>\"),bot_reply])\n",
        "    return history, history\n",
        "\n",
        "chatbot_interface = gradio.Interface(fn=chat_cb,\n",
        "             inputs=[\"text\", \"state\"],\n",
        "             outputs=[\"chatbot\", \"state\"],\n",
        "             title=\"LLaMA-7b Finetuned on /r/\"+SUBREDDIT_NAME,\n",
        "            )\n",
        "chatbot_interface.launch(share=CREATE_TEMPORARY_PUBLIC_CHATBOT_URL_FOR_SHARING)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34b97078-4f8d-4f45-a697-d2f40ae842c4",
      "metadata": {
        "id": "34b97078-4f8d-4f45-a697-d2f40ae842c4"
      },
      "source": [
        "TBC"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}